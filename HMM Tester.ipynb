{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyria\\Miniconda3\\envs\\py3iaml\\lib\\site-packages\\sklearn\\utils\\__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "import nltk, inspect, math, numpy as np\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import map_tag\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# module for computing a Conditional Frequency Distribution\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# module for computing a Conditional Probability Distribution\n",
    "from nltk.probability import ConditionalProbDist\n",
    "\n",
    "from nltk.probability import MLEProbDist\n",
    "from nltk.probability import LidstoneProbDist\n",
    "\n",
    "assert map_tag('brown', 'universal', 'NR-TL') == 'NOUN', '''\n",
    "Brown-to-Universal POS tag map is out of date.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\kyria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words tagged with Penn Treebank POS labels:\n",
      "[('The', 'DET'), ('Fulton', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories='news', tagset='universal')\n",
    "print('Words tagged with Penn Treebank POS labels:')\n",
    "print(tagged_words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tagged with Penn Treebank POS labels:\n",
      "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = brown.tagged_sents(categories='news', tagset='universal')\n",
    "print('Sentence tagged with Penn Treebank POS labels:')\n",
    "print(tagged_sentences[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute emission model using ConditionalProbDist with the estimator:\n",
    "# Lidstone probability distribution with +0.01 added to the sample count for each bin and an extra bin\n",
    "def emission_model(train_data):\n",
    "    \"\"\"\n",
    "    Compute an emission model using a ConditionalProbDist.\n",
    "\n",
    "    :param train_data: The training dataset, a list of sentences with tags\n",
    "    :type train_data: list(list(tuple(str,str)))\n",
    "    :return: The emission probability distribution and a list of the states\n",
    "    :rtype: Tuple[ConditionalProbDist, list(str)]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO prepare data\n",
    "\n",
    "    # Don't forget to lowercase the observation otherwise it mismatches the test data\n",
    "    data = []\n",
    "    for sentence in train_data:\n",
    "        temp = [(tag, word.lower()) for (word, tag) in sentence]\n",
    "        data.extend(temp)\n",
    "    \n",
    "    # TODO compute the emission model\n",
    "    emission_FD = ConditionalFreqDist(data)\n",
    "    emission_PD = ConditionalProbDist(emission_FD, LidstoneProbDist, gamma=0.01)\n",
    "    states = set([tag for (tag, word) in data])\n",
    "\n",
    "    return emission_PD, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission probability distribution:\n",
      "<ConditionalProbDist with 12 conditions>\n",
      "\n",
      "List of states:\n",
      "{'.', 'NOUN', 'CONJ', 'PRON', 'ADV', 'DET', 'X', 'ADP', 'PRT', 'NUM', 'VERB', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "emission_PD, states = emission_model(tagged_sentences)\n",
    "print('Emission probability distribution:')\n",
    "print(emission_PD)\n",
    "print('\\nList of states:')\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transition model using ConditionalProbDist with the estimator:\n",
    "# Lidstone probability distribution with +0.01 added to the sample count for each bin and an extra bin\n",
    "def transition_model(train_data):\n",
    "    \"\"\"\n",
    "    Compute an transition model using a ConditionalProbDist.\n",
    "\n",
    "    :param train_data: The training dataset, a list of sentences with tags\n",
    "    :type train_data: list(list(tuple(str,str)))\n",
    "    :return: The transition probability distribution\n",
    "    :rtype: ConditionalProbDist\n",
    "    \"\"\"\n",
    "    # TODO: prepare the data\n",
    "    data = []\n",
    "\n",
    "    # The data object should be an array of tuples of conditions and observations,\n",
    "    # in our case the tuples will be of the form (tag_(i),tag_(i+1)).\n",
    "    # DON'T FORGET TO ADD THE START SYMBOL </s> and the END SYMBOL </s>\n",
    "    tags = []\n",
    "    for s in train_data:\n",
    "        temp = ['<s>']\n",
    "        temp_tags = [tag for (word, tag) in s]\n",
    "        temp.extend(temp_tags)\n",
    "        temp.append('</s>')\n",
    "        tags.extend(temp)\n",
    "        \n",
    "    data = [(tags[i], tags[i+1]) for i in range(len(tags) - 1)]\n",
    "    # TODO compute the transition model\n",
    "\n",
    "    transition_FD = ConditionalFreqDist(data)\n",
    "    # lidstone_estimator = lambda fd: LidstoneProbDist(fd, 0.01, fd.B() + 1)\n",
    "    # transition_PD = ConditionalProbDist(transition_FD, lidstone_estimator)\n",
    "    transition_PD = ConditionalProbDist(transition_FD, LidstoneProbDist, gamma=0.01)\n",
    "    \n",
    "    return transition_PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test transition_model:\n",
      "start: 1.9362344129159337\n",
      "end: 7.502829039080348\n"
     ]
    }
   ],
   "source": [
    "print('Test transition_model:')\n",
    "transition_PD = transition_model(tagged_sentences)\n",
    "start = -transition_PD['<s>'].logprob('NOUN') #1.78408417115\n",
    "print('start:', start)\n",
    "end = -transition_PD['NOUN'].logprob('</s>') #7.31426021816\n",
    "print('end:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1365887728668209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_PD['NOUN'].prob('VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tagged_sentences\n",
    "\n",
    "\"\"\"\n",
    "Compute an transition model using a ConditionalProbDist.\n",
    "\n",
    ":param train_data: The training dataset, a list of sentences with tags\n",
    ":type train_data: list(list(tuple(str,str)))\n",
    ":return: The transition probability distribution\n",
    ":rtype: ConditionalProbDist\n",
    "\"\"\"\n",
    "\n",
    "# TODO: prepare the data\n",
    "data = []\n",
    "\n",
    "# The data object should be an array of tuples of conditions and observations,\n",
    "# in our case the tuples will be of the form (tag_(i),tag_(i+1)).\n",
    "# DON'T FORGET TO ADD THE START SYMBOL </s> and the END SYMBOL </s>\n",
    "tags = []\n",
    "for s in train_data:\n",
    "    temp = ['<s>']\n",
    "    temp_tags = [tag for (word, tag) in s]\n",
    "    temp.extend(temp_tags)\n",
    "    temp.append('</s>')\n",
    "    tags.extend(temp)\n",
    "        \n",
    "data = [(tags[i], tags[i+1]) for i in range(len(tags) - 1)]\n",
    "# TODO compute the transition model\n",
    "\n",
    "transition_FD = ConditionalFreqDist(data)\n",
    "# lidstone_estimator = lambda fd: LidstoneProbDist(fd, 0.01, fd.B() + 1)\n",
    "# transition_PD = ConditionalProbDist(transition_FD, lidstone_estimator)\n",
    "transition_PD = ConditionalProbDist(transition_FD, LidstoneProbDist, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('<s>', 'DET'), ('DET', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'ADJ'), ('ADJ', 'NOUN'), ('NOUN', 'VERB'), ('VERB', 'NOUN'), ('NOUN', 'DET'), ('DET', 'NOUN'), ('NOUN', 'ADP'), ('ADP', 'NOUN'), ('NOUN', 'ADJ'), ('ADJ', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'VERB'), ('VERB', '.'), ('.', 'DET'), ('DET', 'NOUN'), ('NOUN', '.'), ('.', 'ADP'), ('ADP', 'DET'), ('DET', 'NOUN'), ('NOUN', 'VERB'), ('VERB', 'NOUN'), ('NOUN', '.'), ('.', '</s>'), ('</s>', '<s>'), ('<s>', 'DET'), ('DET', 'NOUN'), ('NOUN', 'ADV'), ('ADV', 'VERB'), ('VERB', 'ADP'), ('ADP', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'ADP'), ('ADP', 'DET'), ('DET', 'NOUN'), ('NOUN', 'ADJ'), ('ADJ', 'NOUN'), ('NOUN', '.'), ('.', 'DET'), ('DET', 'VERB'), ('VERB', 'ADJ'), ('ADJ', 'NOUN'), ('NOUN', 'ADP'), ('ADP', 'DET'), ('DET', 'NOUN'), ('NOUN', '.'), ('.', '.'), ('.', 'VERB'), ('VERB', 'DET'), ('DET', 'NOUN'), ('NOUN', 'CONJ'), ('CONJ', 'NOUN'), ('NOUN', 'ADP'), ('ADP', 'DET'), ('DET', 'NOUN'), ('NOUN', 'ADP'), ('ADP', 'NOUN'), ('NOUN', '.'), ('.', 'ADP'), ('ADP', 'DET'), ('DET', 'NOUN'), ('NOUN', 'ADP'), ('ADP', 'DET'), ('DET', 'DET'), ('DET', 'NOUN'), ('NOUN', 'VERB'), ('VERB', 'VERB'), ('VERB', '.'), ('.', '</s>'), ('</s>', '<s>'), ('<s>', 'DET'), ('DET', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'VERB'), ('VERB', 'VERB'), ('VERB', 'VERB'), ('VERB', 'ADP'), ('ADP', 'NOUN'), ('NOUN', 'ADJ'), ('ADJ', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'NOUN'), ('NOUN', 'PRT'), ('PRT', 'VERB'), ('VERB', 'NOUN'), ('NOUN', 'ADP'), ('ADP', 'ADJ'), ('ADJ', '.'), ('.', 'NOUN'), ('NOUN', '.'), ('.', 'ADP'), ('ADP', 'DET'), ('DET', 'ADJ'), ('ADJ', 'NOUN'), ('NOUN', 'DET'), ('DET', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(data[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.ConditionalProbDist'>\n",
      "['<s>', 'DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', '</s>', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']\n"
     ]
    }
   ],
   "source": [
    "print(type(transition_PD))\n",
    "print(transition_PD.conditions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, train_data, test_data):\n",
    "        \"\"\"\n",
    "        Initialise a new instance of the HMM.\n",
    "\n",
    "        :param train_data: The training dataset, a list of sentences with tags\n",
    "        :type train_data: list(list(tuple(str,str)))\n",
    "        :param test_data: the test/evaluation dataset, a list of sentence with tags\n",
    "        :type test_data: list(list(tuple(str,str)))\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        # Emission and transition probability distributions\n",
    "        self.emission_PD: ConditionalProbDist = None\n",
    "        self.transition_PD: ConditionalProbDist = None\n",
    "        self.states = []\n",
    "\n",
    "        self.viterbi = []\n",
    "        self.backpointer = []\n",
    "\n",
    "    # Compute emission model using ConditionalProbDist with the estimator:\n",
    "    # Lidstone probability distribution with +0.01 added to the sample count for each bin and an extra bin\n",
    "    def emission_model(self, train_data):\n",
    "        \"\"\"\n",
    "        Compute an emission model using a ConditionalProbDist.\n",
    "\n",
    "        :param train_data: The training dataset, a list of sentences with tags\n",
    "        :type train_data: list(list(tuple(str,str)))\n",
    "        :return: The emission probability distribution and a list of the states\n",
    "        :rtype: Tuple[ConditionalProbDist, list(str)]\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO prepare data\n",
    "\n",
    "        # Don't forget to lowercase the observation otherwise it mismatches the test data\n",
    "        data = []\n",
    "        for s in train_data:\n",
    "            temp = [(tag, word.lower()) for (word, tag) in s]\n",
    "            data.extend(temp)\n",
    "                \n",
    "        # TODO compute the emission model\n",
    "        emission_FD = ConditionalFreqDist(data)\n",
    "        self.emission_PD = ConditionalProbDist(emission_FD, LidstoneProbDist, gamma=0.01)\n",
    "        #self.states = [u'.', u'ADJ', u'ADP', u'ADV', u'CONJ', u'DET', u'NOUN', u'NUM', u'PRON', u'PRT', u'VERB', u'X']\n",
    "        #self.states = ['NOUN']\n",
    "        \n",
    "        state_list = []\n",
    "        for w in data:\n",
    "            state_list.append(w[0])\n",
    "            \n",
    "        self.states = list(set(state_list))\n",
    "        \n",
    "        return self.emission_PD, self.states\n",
    "\n",
    "    # Compute transition model using ConditionalProbDist with the estimator:\n",
    "    # Lidstone probability distribution with +0.01 added to the sample count for each bin and an extra bin\n",
    "    def transition_model(self, train_data):\n",
    "        \"\"\"\n",
    "        Compute an transition model using a ConditionalProbDist.\n",
    "\n",
    "        :param train_data: The training dataset, a list of sentences with tags\n",
    "        :type train_data: list(list(tuple(str,str)))\n",
    "        :return: The transition probability distribution\n",
    "        :rtype: ConditionalProbDist\n",
    "        \"\"\"\n",
    "        # TODO: prepare the data\n",
    "        data = []\n",
    "\n",
    "        # The data object should be an array of tuples of conditions and observations,\n",
    "        # in our case the tuples will be of the form (tag_(i),tag_(i+1)).\n",
    "        # DON'T FORGET TO ADD THE START SYMBOL </s> and the END SYMBOL </s>\n",
    "        tags = []\n",
    "        for s in train_data:\n",
    "            temp = ['<s>']\n",
    "            temp_tags = [tag for (word, tag) in s]\n",
    "            temp.extend(temp_tags)\n",
    "            temp.append('</s>')\n",
    "            tags.extend(temp)\n",
    "        \n",
    "        data = [(tags[i], tags[i+1]) for i in range(len(tags) - 1)]\n",
    "\n",
    "        # TODO compute the transition model\n",
    "\n",
    "        transition_FD = ConditionalFreqDist(data)\n",
    "        self.transition_PD = ConditionalProbDist(transition_FD, LidstoneProbDist, gamma=0.01)\n",
    "\n",
    "        return self.transition_PD\n",
    "\n",
    "    # Train the HMM\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the HMM from the training data\n",
    "        \"\"\"\n",
    "        self.emission_model(self.train_data)\n",
    "        self.transition_model(self.train_data)\n",
    "\n",
    "    # Part B: Implementing the Viterbi algorithm.\n",
    "\n",
    "    # Initialise data structures for tagging a new sentence.\n",
    "    # Describe the data structures with comments.\n",
    "    # Use the models stored in the variables: self.emission_PD and self.transition_PD\n",
    "    # Input: first word in the sentence to tag\n",
    "    def initialise(self, observation):\n",
    "        \"\"\"\n",
    "        Initialise data structures for tagging a new sentence.\n",
    "\n",
    "        :param observation: the first word in the sentence to tag\n",
    "        :type observation: str\n",
    "        \"\"\"\n",
    "        # Initialise viterbi, including\n",
    "        #  transition from <s> to observation\n",
    "        # use costs (-log-base-2 probabilities)\n",
    "        \n",
    "        # The viterbi data structure contains the Viterbi path probabilities as a T by N\n",
    "        # table where T is the number of observations and N is the number of states or tags.\n",
    "        # Each cell contains the most probable path by taking the maximum over all possible \n",
    "        # previous state sequences to arrive at that state.\n",
    "        self.viterbi = []\n",
    "        \n",
    "        # The backpointer data structure keeps track of the best path of hidden states that \n",
    "        # led to each state in a T by N table where  is the number of observations and N is \n",
    "        # the number of states. Each cell contains the state which had the maximum viterbi\n",
    "        # probability (from the Viterbi table) in the previous time step (or observation).\n",
    "        self.backpointer = []\n",
    "        self.viterbi.append([])\n",
    "        self.backpointer.append([])\n",
    "        \n",
    "        for state in self.states:\n",
    "            self.viterbi[0].extend([- math.log2(self.transition_PD[\"<s>\"].prob(state)) - math.log2(self.emission_PD[state].prob(observation))])\n",
    "            # Initialise backpointer\n",
    "            self.backpointer[0].extend(['<s>'])                   \n",
    "          \n",
    "    # Tag a new sentence using the trained model and already initialised data structures.\n",
    "    # Use the models stored in the variables: self.emission_PD and self.transition_PD.\n",
    "    # Update the self.viterbi and self.backpointer datastructures.\n",
    "    # Describe your implementation with comments.\n",
    "    # Input: list of words\n",
    "    def tag(self, observations):\n",
    "        \"\"\"\n",
    "        Tag a new sentence using the trained model and already initialised data structures.\n",
    "\n",
    "        :param observations: List of words (a sentence) to be tagged\n",
    "        :type observations: list(str)\n",
    "        :return: List of tags corresponding to each word of the input\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "        index = 0\n",
    "        current_decision = []\n",
    "\n",
    "        for t in range(1, len(observations)):\n",
    "            #print('\\nPrevious observation: ', observations[t-1])\n",
    "            #print('\\nCurrent obesrvation: ', observations[t])\n",
    "            \n",
    "            # Appending a new list for storing the current Viterbi costs.\n",
    "            self.viterbi.append([])\n",
    "            \n",
    "            # Getting the Viterbi costs of the previous observation.\n",
    "            previous_viterbi = np.array(self.viterbi[t-1])\n",
    "            \n",
    "            # Appending a new list for storing the backpointers for the states of this observation.\n",
    "            self.backpointer.append([])\n",
    "            #print('\\nPrevious Viterbi column: ')\n",
    "            #print(previous_viterbi)\n",
    "\n",
    "            # Double recursion to calculate the best paths for each state from all other states.  \n",
    "            for i in range(len(self.states)):\n",
    "                current_state = self.states[i]\n",
    "                #print('Current state: ')\n",
    "                #print(current_state)\n",
    "                \n",
    "                current_state_transition = []\n",
    "                \n",
    "                for j in range(len(self.states)):                    \n",
    "                    previous_state = self.states[j]\n",
    "                    #print('Previous state: ')\n",
    "                    #print(previous_state)\n",
    "                    \n",
    "                    # Getting the transition cost from the previous state to the current state and storing it.\n",
    "                    state_transition_cost = -math.log2(self.transition_PD[previous_state].prob(current_state))\n",
    "                    current_state_transition.append(state_transition_cost)\n",
    "          \n",
    "                # Getting the state observation cost.\n",
    "                emission_cost = - math.log2(self.emission_PD[current_state].prob(observations[t]))\n",
    "                #print('Emission cost: ')\n",
    "                #print(emission_cost)\n",
    "            \n",
    "                # Transforming the current_state_transition list to a numpy array.\n",
    "                current_state_transition = np.array(current_state_transition)     \n",
    "                #print('Transtions probabilities:')\n",
    "                #print(current_state_transition)\n",
    "                \n",
    "                # Calculating the Viterbi costs from each previous state to the current state using element-wise addition.\n",
    "                current_state_costs = previous_viterbi + current_state_transition + emission_cost\n",
    "                #print('State costs:')\n",
    "                #print(current_state_costs)\n",
    "                                               \n",
    "                #print('\\nMinimum state: ', self.states[np.argmin(current_state_costs)])\n",
    "                \n",
    "                # Calculating and storing the minimum Viterbi cost for the current state.\n",
    "                self.viterbi[t].append(np.amin(current_state_costs))\n",
    "                \n",
    "                # Calculating and storing the backpointer for the current state.\n",
    "                self.backpointer[t].append(np.argmin(current_state_costs))\n",
    "                     \n",
    "        # TODO\n",
    "        # Add cost of termination step (for transition to </s> , end of sentence).\n",
    "        \n",
    "        # Getting the Viterbi costs of the last observation.\n",
    "        previous_viterbi = np.array(self.viterbi[-1])\n",
    "        \n",
    "        # Appending a new list for storing the Viterbi costs to the end </s> state.\n",
    "        self.viterbi.append([])\n",
    "        \n",
    "        # Appending a new list for storing the backpointers for the states of the end </s> state.\n",
    "        self.backpointer.append([])\n",
    "        \n",
    "        current_state_transition = []\n",
    "        \n",
    "        # Recursion over the states of the final observation.\n",
    "        for i in range(len(self.states)):\n",
    "            previous_state = self.states[i]\n",
    "            \n",
    "            # Getting and storing the transition cost from the final observation state to the end </s> state.\n",
    "            state_transition_cost = -math.log2(self.transition_PD[previous_state].prob('</s>'))\n",
    "            current_state_transition.append(state_transition_cost)\n",
    "\n",
    "        # Transforming the current_state_transition list to a numpy array.\n",
    "        current_state_transition = np.array(current_state_transition)\n",
    "         \n",
    "        # Calculating the Viterbi costs from each previous state to the end state using element-wise addition.\n",
    "        current_state_costs = previous_viterbi + current_state_transition\n",
    "    \n",
    "        # Calculating and storing the minimum Viterbi cost for the end state.\n",
    "        self.viterbi[len(observations)].append(np.amin(current_state_costs))\n",
    "        \n",
    "        # Calculating and storing the backpointer for the end state.\n",
    "        self.backpointer[len(observations)].append(np.argmin(current_state_costs))\n",
    "        \n",
    "        #print('\\nObservations: ', observations)\n",
    "        #print('\\nViterbi table length: ', len(self.viterbi))\n",
    "        #print('\\nViterbi table:')\n",
    "        #print(self.viterbi)\n",
    "        #print('\\nBackpointer table length: ', len(self.backpointer))\n",
    "        #print('\\nBackpointer table:')\n",
    "        #print(self.backpointer)\n",
    "        \n",
    "        # TODO\n",
    "        # Reconstruct the tag sequence using the backpointer list.\n",
    "        # Return the tag sequence corresponding to the best path as a list.\n",
    "        # The order should match that of the words in the sentence.\n",
    "        tag_index = self.backpointer[len(observations)][0]\n",
    "        tags.insert(0, self.states[tag_index])\n",
    "        \n",
    "        for i in reversed(range(1, len(observations))):\n",
    "            tag_index = self.backpointer[i][tag_index]\n",
    "            tags.insert(0, self.states[tag_index])\n",
    "            \n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question4b():\n",
    "    \"\"\" Report a tagged sequence that is incorrect\n",
    "    :rtype: str\n",
    "    :return: your answer [max 280 chars]\"\"\"\n",
    "    \n",
    "    tagged_sequence = []\n",
    "    correct_sequence = []\n",
    "    \n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    model = HMM(train_data_universal, test_data_universal)\n",
    "    model.train()\n",
    "    \n",
    "    for sentence in test_data_universal:\n",
    "        s = [word.lower() for (word, tag) in sentence]\n",
    "        model.initialise(s[0])\n",
    "        tags = model.tag(s)\n",
    "\n",
    "        count = 0\n",
    "            \n",
    "        for ((word,gold),tag) in zip(sentence,tags):\n",
    "            #print('\\nSentence length: ', len(sentence))\n",
    "            #print('\\nSentence: ', sentence)\n",
    "            \n",
    "            #print('\\nTags: ', tags)\n",
    "            \n",
    "            \n",
    "            if tag == gold:\n",
    "                count += 1\n",
    "                #correct += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        if count != len(sentence):\n",
    "            tagged_sequence.append(tags)\n",
    "            correct_sequence.append([tag for (word, tag) in sentence])\n",
    "            \n",
    "        if len(tagged_sequence) == 10:\n",
    "            break\n",
    "    \n",
    "    # Why do you think the tagger tagged this example incorrectly?\n",
    "    answer =  inspect.cleandoc(\"\"\"There is an incorrect adjective tag on the word \\'Fulton\\', while the correct tag is a noun. This is due to the next word being a noun and the probability of a noun following an adjective is higher than a noun following a noun.\"\"\")[0:280]\n",
    "    \n",
    "    return tagged_sequence[0], correct_sequence[0], answer\n",
    "\n",
    "def answer_question5():\n",
    "    \"\"\"Suppose you have a hand-crafted grammar that has 100% coverage on\n",
    "        constructions but less than 100% lexical coverage.\n",
    "        How could you use a POS tagger to ensure that the grammar\n",
    "        produces a parse for any well-formed sentence,\n",
    "        even when it doesn't recognise the words within that sentence?\n",
    "\n",
    "    :rtype: str\n",
    "    :return: your answer [max 500 chars]\"\"\"\n",
    "\n",
    "    return inspect.cleandoc(\"\"\"\\\n",
    "    fill me in\"\"\")[0:500]\n",
    "\n",
    "# Useful for testing\n",
    "def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n",
    "    # http://stackoverflow.com/a/33024979\n",
    "    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for testing\n",
    "def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n",
    "    # http://stackoverflow.com/a/33024979\n",
    "    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n",
    "\n",
    "def answers():\n",
    "    global tagged_sentences_universal, test_data_universal, \\\n",
    "           train_data_universal, model, test_size, train_size, ttags, \\\n",
    "           correct, incorrect, accuracy, \\\n",
    "           good_tags, bad_tags, answer4b, answer5\n",
    "           \n",
    "    # Test of model for sentence with single word 'sentence':\n",
    "    print('\\nSingle word sentence test: ')\n",
    "    train_test_s = [[('sentence', 'NOUN')]]\n",
    "    model = HMM(train_test_s, train_test_s)\n",
    "    model.train()\n",
    "\n",
    "    s = 'sentence'.split()\n",
    "    model.initialise(s[0])\n",
    "    ttags = model.tag(s)\n",
    "    print('\\nEmission probability of word \\'sentence\\' being a noun: ', model.emission_PD['NOUN'].prob('sentence'))\n",
    "    print('Transition probability from start of sentence to noun: ', model.transition_PD['<s>'].prob('NOUN'))\n",
    "    print('Transition probability from noun of sentence to noun: ', model.transition_PD['NOUN'].prob('NOUN'))\n",
    "    print('\\nTagging of the single word sentence by the model:')\n",
    "    print(list(zip(s,ttags)))\n",
    "\n",
    "\n",
    "    # Test of model for sentence with two 'sentence' words:\n",
    "    print('\\n\\nDouble word sentence test: ')\n",
    "    train_test_s = [[('sentence', 'NOUN'), ('sentence', 'NOUN')]]\n",
    "    model = HMM(train_test_s, train_test_s)\n",
    "    model.train()\n",
    "\n",
    "    s = 'sentence sentence'.split()\n",
    "    model.initialise(s[0])\n",
    "    ttags = model.tag(s)\n",
    "    print('\\nEmission probability of word \\'sentence\\' being a noun: ', model.emission_PD['NOUN'].prob('sentence'))\n",
    "    print('Transition probability from start of sentence to noun: ', model.transition_PD['<s>'].prob('NOUN'))\n",
    "    print('Transition probability from noun of sentence to noun: ', model.transition_PD['NOUN'].prob('NOUN'))\n",
    "    print('\\nTagging of the double word sentence by the model:')\n",
    "    print(list(zip(s,ttags)))\n",
    "    \n",
    "    print('\\n\\nModel Test on universal tagset: ')\n",
    "    # Load the Brown corpus with the Universal tag set.\n",
    "    tagged_sentences_universal = brown.tagged_sents(categories='news', tagset='universal')\n",
    "\n",
    "    # Divide corpus into train and test data.\n",
    "    test_size = 1000\n",
    "    #train_size = len(tagged_sentences_universal) - 1000\n",
    "    train_size = len(tagged_sentences_universal)\n",
    "\n",
    "    #test_data_universal = tagged_sentences_universal[(-test_size):]\n",
    "    #train_data_universal = tagged_sentences_universal[:train_size]\n",
    "    \n",
    "    test_data_universal = tagged_sentences_universal[0:test_size]\n",
    "    train_data_universal = tagged_sentences_universal[test_size:train_size]\n",
    "    \n",
    "    # Create instance of HMM class and initialise the training and test sets.\n",
    "    model = HMM(train_data_universal, test_data_universal)\n",
    "\n",
    "    # Train the HMM.\n",
    "    model.train()\n",
    "\n",
    "    # Inspect the model to see if emission_PD and transition_PD look plausible\n",
    "    print('\\nstates: %s\\n'%model.states)\n",
    "    \n",
    "    # Added checks:\n",
    "    print('Probability of \\'are\\' being a VERB: ', emission_PD['VERB'].prob('are'))\n",
    "    print('Probability of \\'the\\' being a DET: ', emission_PD['DET'].prob('the'))\n",
    "    print('Probability of a noun followed by an adjective: ', transition_PD['NOUN'].prob('ADJ'))\n",
    "    print('Probability of an adjective followed by a noun: ', transition_PD['ADJ'].prob('NOUN'))\n",
    "    \n",
    "    ######\n",
    "    # Try the model, and test its accuracy [won't do anything useful\n",
    "    #  until you've filled in the tag method\n",
    "    ######\n",
    "    s='the cat in the hat came back'.split()\n",
    "    model.initialise(s[0])\n",
    "    ttags = model.tag(s)\n",
    "    print('\\nTag a trial sentence')\n",
    "    print(list(zip(s,ttags)))\n",
    "\n",
    "    # check the model's accuracy (% correct) using the test set\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for sentence in test_data_universal:\n",
    "        s = [word.lower() for (word, tag) in sentence]\n",
    "        model.initialise(s[0])\n",
    "        tags = model.tag(s)\n",
    "\n",
    "        for ((word,gold),tag) in zip(sentence,tags):            \n",
    "            if tag == gold:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1              \n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    print('\\nTagging accuracy for test set of %s sentences: %.4f'%(test_size,accuracy))\n",
    "\n",
    "    # Print answers for 4b and 5\n",
    "    bad_tags, good_tags, answer4b = answer_question4b()\n",
    "    print('\\nAn incorrect tagged sequence is:')\n",
    "    print(bad_tags)\n",
    "    print('\\nThe correct tagging of this sentence would be:')\n",
    "    print(good_tags)\n",
    "    print('\\nA possible reason why this error may have occurred is:')\n",
    "    print(answer4b[:280])\n",
    "    answer5=answer_question5()\n",
    "    print('\\nFor Q5:')\n",
    "    print(answer5[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single word sentence test: \n",
      "\n",
      "Emission probability of word 'sentence' being a noun:  1.0\n",
      "Transition probability from start of sentence to noun:  1.0\n",
      "Transition probability from noun of sentence to noun:  0.009900990099009901\n",
      "\n",
      "Tagging of the single word sentence by the model:\n",
      "[('sentence', 'NOUN')]\n",
      "\n",
      "\n",
      "Double word sentence test: \n",
      "\n",
      "Emission probability of word 'sentence' being a noun:  1.0\n",
      "Transition probability from start of sentence to noun:  1.0\n",
      "Transition probability from noun of sentence to noun:  0.5\n",
      "\n",
      "Tagging of the double word sentence by the model:\n",
      "[('sentence', 'NOUN'), ('sentence', 'NOUN')]\n",
      "\n",
      "\n",
      "Model Test on universal tagset: \n",
      "\n",
      "states: ['.', 'NOUN', 'CONJ', 'PRON', 'ADV', 'DET', 'X', 'ADP', 'PRT', 'NUM', 'VERB', 'ADJ']\n",
      "\n",
      "Probability of 'are' being a VERB:  0.02287371044700807\n",
      "Probability of 'the' being a DET:  0.5607025892723873\n",
      "Probability of a noun followed by an adjective:  0.016898538630846806\n",
      "Probability of an adjective followed by a noun:  0.7095016052477361\n",
      "\n",
      "Tag a trial sentence\n",
      "[('the', 'DET'), ('cat', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('hat', 'NOUN'), ('came', 'VERB'), ('back', 'ADV')]\n",
      "\n",
      "Tagging accuracy for test set of 1000 sentences: 0.8955\n",
      "\n",
      "An incorrect tagged sequence is:\n",
      "['DET', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'VERB', '.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.']\n",
      "\n",
      "The correct tagging of this sentence would be:\n",
      "['DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', '.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.']\n",
      "\n",
      "A possible reason why this error may have occurred is:\n",
      "There is an incorrect adjective tag on the word 'Fulton', while the correct tag is a noun. This is due to the next word being a noun and the probability of a noun following an adjective is higher than a noun following a noun.\n",
      "\n",
      "For Q5:\n",
      "fill me in\n"
     ]
    }
   ],
   "source": [
    "answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
